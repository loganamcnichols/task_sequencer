{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Sequencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>\"Sometimes when you innovate, you make mistakes. It is best to admit them quickly, and get on with improving your other innovations.\"<i> -Steve Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tool which will calculate the optimal order to approach a project where there are several different tasks that need to be done that have some probability of failing.\n",
    "\n",
    "When we are uncertain about the feasibility of a project, we should strive to get as much information about the difficulty as quickly as possible. This suggests that we should start with tasks with a high failure rate.\n",
    "\n",
    "The idea behind this approach is and some other useful information is discussed here. https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import ipysheet\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "from IPython.display import Markdown, display, clear_output, FileLink\n",
    "from symbulate import RV, Exponential\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "def df_from_sheet(sheet):\n",
    "    '''Converts the sheet to a compact dataframe'''\n",
    "    df = ipysheet.to_dataframe(sheet)\n",
    "    df = fill_blanks(df)\n",
    "    df = trim_df(df)\n",
    "    df = switch_from_names(df)\n",
    "    df = nums_to_float(df)\n",
    "    df = switch_to_names(df)\n",
    "    return df\n",
    "\n",
    "def switch_to_names(df):\n",
    "    '''Converts indicies in the dependency cols\n",
    "    to their corresponding task names'''\n",
    "    df.iloc[:,3:] = df.iloc[:,3:].replace(\n",
    "        dict(zip(df.index + 1, df['Task'])))\n",
    "    return df\n",
    "\n",
    "def switch_from_names(df):\n",
    "    '''Converts the task names  in the dependency \n",
    "    cols to their coresponding indicies'''\n",
    "    df.iloc[:,3:] = df.iloc[:,3:].replace(\n",
    "        names_to_index_dict(df))\n",
    "    return df\n",
    "\n",
    "def names_to_index_dict(df):\n",
    "    return dict(zip(df['Task'], df.index + 1))\n",
    "\n",
    "def fill_blanks(df):\n",
    "    '''Replaces NaN and empty values with 0s'''\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.replace([\"\", \"0\"], 0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def trim_df(df):\n",
    "    '''Removes rows and columns that are blank (\n",
    "    or contain zeros)'''\n",
    "    zero_row_bool = (df.values==0).all(axis=1)\n",
    "    zero_row_loc = np.where(zero_row_bool)[0]\n",
    "    if zero_row_loc.size == 0:\n",
    "        first_zero_row=df.shape[0]\n",
    "    else:\n",
    "        first_zero_row = zero_row_loc[0]\n",
    "    zero_col_bool = (df.values==0).all(axis=0)\n",
    "    zero_col_loc = np.where(zero_col_bool)[0]\n",
    "    if zero_col_loc.size == 0:\n",
    "        first_zero_col = df.shape[1]\n",
    "    else:\n",
    "        first_zero_col = zero_col_loc[0]\n",
    "    return df.iloc[:first_zero_row,:first_zero_col]\n",
    "    \n",
    "def nums_to_float(df):\n",
    "    '''Converts all numerical entries\n",
    "    to floats'''\n",
    "    df.iloc[:,1:] = df.iloc[:,1:].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_sheet(task_report=\"task_sheet.csv\",\n",
    "               rows=10, cols=10):\n",
    "    '''Creates the task dependency sheet'''\n",
    "    labels = (['Task', 'Time', 'Probability'] \n",
    "              + [\"D\" + str(i + 1) \n",
    "                 for i in range(cols - 3)])\n",
    "    sheet = sheet_from_df(task_report, rows, \n",
    "                          cols, labels)\n",
    "    if sheet:\n",
    "        return sheet\n",
    "    sheet = sheet_from_scratch(rows, cols, labels)\n",
    "    return sheet\n",
    "\n",
    "def sheet_from_df(task_report, rows, \n",
    "                  cols, labels):\n",
    "    '''Returns a dataframe from task_report if\n",
    "    task_report is in the working directory. Otherwise\n",
    "    returns None.'''\n",
    "    try:\n",
    "        sheet = ipysheet.sheet(\n",
    "            rows=rows,\n",
    "            columns=cols,\n",
    "            column_headers=labels)\n",
    "        df = pd.read_csv(task_report)\n",
    "        df = rid_zeros(df)\n",
    "        data = np.empty([rows, cols], dtype=object)\n",
    "        data[:df.shape[0], :df.shape[1]] = df.values\n",
    "        ipysheet.cell_range(data)\n",
    "        return sheet\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def rid_zeros(df):\n",
    "    '''Converts the task names  in the dependency \n",
    "    cols to their coresponding indicies'''\n",
    "    df.iloc[:,3:] = df.iloc[:,3:].replace({'0.0' : None})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def sheet_from_scratch(rows, cols, labels):\n",
    "    '''Generates a sheet with all empty values'''\n",
    "    sheet = ipysheet.sheet(\n",
    "        rows=rows,\n",
    "        columns=cols,\n",
    "        column_headers=labels)\n",
    "    data = [['' for i in range(cols)] \n",
    "            for i in range(rows)]\n",
    "    ipysheet.cell_range(data)\n",
    "    return sheet\n",
    "\n",
    "def gen_perms(df):\n",
    "    '''Creates a list of all permutations of the\n",
    "    rows of the dataframe (indexing starting at 1)'''\n",
    "    rows=df.shape[0]\n",
    "    return permutations(range(1,rows+1))\n",
    "\n",
    "def validate_perms(perms, deps):\n",
    "    '''Returns the rows in perms which dont\n",
    "    violate a dependency'''\n",
    "    valid_perms = []\n",
    "    for perm in perms:\n",
    "        if check_perm(perm, deps) == True:\n",
    "            valid_perms.append(perm)\n",
    "    return valid_perms\n",
    "\n",
    "def check_perm(perm, deps):\n",
    "    '''Returns True or False according to whether\n",
    "    perm violates a dependency'''\n",
    "    prev_ind = [0]\n",
    "    for index in perm:\n",
    "        prev_ind.append(index)\n",
    "        dep = deps[index-1, :]\n",
    "        sub_true = all(elem in prev_ind for elem in dep)\n",
    "        if sub_true is False:\n",
    "            return False\n",
    "        elif index == perm[-1]:\n",
    "            return True\n",
    "\n",
    "def sort_rates(times, probs):\n",
    "    '''Returns an order list of the\n",
    "    failure rates (called lambda in the\n",
    "    appendix)'''\n",
    "    return (np.flip(np.argsort(\n",
    "        1/times*np.log(1/(1-probs)))) + 1)\n",
    "        \n",
    "def get_expected_times(perms, df):\n",
    "    '''Calculates the expected time for each\n",
    "    order of the tasks according to the\n",
    "    equation in appendix A'''\n",
    "    sums = []\n",
    "    for perm in perms:\n",
    "        prob_col = df['Probability'][np.array(perm)-1].values.astype(float)\n",
    "        prob_comp = 1 - prob_col\n",
    "        prob_mat = expand_to_lower_tri_mat(prob_comp)\n",
    "        prod_arr = prob_prod_arr(prob_mat, prob_col)\n",
    "        time_col = df['Time'][np.array(perm) - 1].values.astype(float)\n",
    "        time_mat = expand_to_lower_tri_mat(time_col)\n",
    "        sum_arr = sum_times(time_mat)\n",
    "        sums.append(expected_time_eq(prod_arr,\n",
    "                                     sum_arr, time_col,\n",
    "                                     prob_col, prob_comp))\n",
    "    return sums\n",
    "\n",
    "\n",
    "def expected_time_eq(prod_arr, sum_arr, time_col, prob_col,\n",
    "                     prob_comp):\n",
    "    prob_succ = np.prod(prob_comp)\n",
    "    total_time = np.sum(time_col)\n",
    "    exp_time_given_fail_on_task = (sum_arr\n",
    "                                   - time_col/prob_col\n",
    "                                   + time_col/np.log(1/(1 - prob_col))\n",
    "                                   + time_col)\n",
    "    prob_fail_on_task = prod_arr\n",
    "    return (prob_succ*total_time\n",
    "           + np.sum(prob_fail_on_task*exp_time_given_fail_on_task))\n",
    "\n",
    "\n",
    "\n",
    "def expand_to_lower_tri_mat(arr):\n",
    "    '''Converts arr to a lower triangular matrix.\n",
    "    e.g. [1,2,3] => [[1,0,0]\n",
    "                     [1,2,0]\n",
    "                     [1,2,3]]'''\n",
    "    two_d_arr = np.expand_dims((arr), 0)\n",
    "    square_mat = np.repeat(two_d_arr,\n",
    "                             repeats=arr.shape[0],\n",
    "                             axis=0)\n",
    "    return np.tril(square_mat)\n",
    "    \n",
    "def prob_prod_arr(prob_mat, prob_col):\n",
    "    '''Returns the product array that will satisfy the\n",
    "    equation in the appendix e.g. \n",
    "    prob_mat = [[0.8, 0.0, 0.0],    prob_col = [0.2, 0.3, 0.4]\n",
    "                [0.8, 0.7, 0.0],\n",
    "                [0.8, 0.7, 0.6]]\n",
    "    => prod_arr = column product of( [[0.8, 0.0, 0.0], \n",
    "                                     [0.8, 0.7, 0.0],\n",
    "                                     [0.8, 0.7, 0.6]]                    \n",
    "                                    +[[0.2, 0.0, 0.0],\n",
    "                                      [0.0, 0.3, 0.0],\n",
    "                                      [0.0, 0.0, 0.4]] \n",
    "                                    +[[0.0, 1.0, 1,0],\n",
    "                                      [0.0, 0.0, 1.0],\n",
    "                                      [0.0, 0.0, 0.0]] )\n",
    "      which equals [0.2  , 0.24 , 0.224]'''                              \n",
    "    np.fill_diagonal(prob_mat, 0)\n",
    "    ones = np.triu(np.ones(np.shape(prob_mat)))\n",
    "    np.fill_diagonal(ones, 0)\n",
    "    return np.prod(ones + prob_mat + np.diag(prob_col), 1)\n",
    "\n",
    "def sum_times(time_mat):\n",
    "    '''Returns an array of sums from\n",
    "    a lower triangular matrix after zeroing\n",
    "    diagonal'''\n",
    "    np.fill_diagonal(time_mat, 0)\n",
    "    return np.sum(time_mat, 1)\n",
    "\n",
    "def adj_transposes(arr):\n",
    "    \"\"\"returns a matrix where each row\n",
    "    is one of the n-1 adjacent transposes\"\"\"\n",
    "    reps_mat = repeat_array_as_matrix(arr, len(arr)-1)\n",
    "    for i in range(len(arr)-1):\n",
    "        ith_row = reps_mat[i,:]\n",
    "        ith_row[i], ith_row[i+1] = ith_row[i+1], ith_row[i]\n",
    "    return reps_mat\n",
    "\n",
    "def find_kids(row, arr, deps):\n",
    "    '''Returns the inidicies of srtd_rates where\n",
    "    the dependents of task in \"row\" are located'''\n",
    "    child_rows = np.argwhere(deps==row)[:,0] + 1\n",
    "    child_loc = np.where(np.in1d(arr, child_rows))[0]\n",
    "    return child_loc\n",
    "\n",
    "def repeat_array_as_matrix(arr, reps):\n",
    "    '''e.g. array([1,2]) to array([[1,2],\n",
    "                                    [1,2]])'''\n",
    "    matrix_head = np.expand_dims(arr, axis=0)\n",
    "    return np.repeat(matrix_head,\n",
    "                      repeats=reps,\n",
    "                        axis=0)\n",
    "\n",
    "def find_parents(row, arr, deps):\n",
    "    '''Returns the inidicies of arr where\n",
    "    the tasks that task in row is dependent on\n",
    "    are located'''\n",
    "    dep = deps[row-1] #minus 1 cause index\n",
    "    parent_rows = dep[np.nonzero(dep)] #rid zeros\n",
    "    parent_loc = np.where(np.in1d(arr, parent_rows))[0]\n",
    "    return parent_loc\n",
    "\n",
    "def calc_disarray(arr, deps):\n",
    "    '''returns a number according to how disarrayed\n",
    "    arr is'''\n",
    "    total_disarray = 0\n",
    "    for task_loc, task in enumerate(arr):\n",
    "        task_kids_loc = find_kids(task, arr, deps)\n",
    "        task_pars_loc = find_parents(task, arr, deps)\n",
    "        total_disarray += element_disarray(task_kids_loc,\n",
    "                                           task_pars_loc,\n",
    "                                           task_loc)\n",
    "    return total_disarray\n",
    "\n",
    "def find_least_disarrayed(mat, deps):\n",
    "    '''Returns the rows of mat that have the\n",
    "    lowest disarray score'''\n",
    "    disarray_counts = np.zeros(mat.shape[0])\n",
    "    for i, row in enumerate(mat):\n",
    "        disarray_counts[i] = calc_disarray(row, deps)\n",
    "    min_disarray = np.amin(disarray_counts)\n",
    "    min_ind = np.where(disarray_counts==min_disarray)[0]\n",
    "    return mat[min_ind,:]\n",
    "\n",
    "def element_disarray(kid_loc, par_loc, loc):\n",
    "    '''Returns a number according to how \"out of place\"\n",
    "    an element is'''\n",
    "    kids_to_left = np.extract(kid_loc<loc, kid_loc)\n",
    "    pars_to_right = np.extract(par_loc>loc, par_loc)\n",
    "    return (np.sum(loc - kids_to_left) + \n",
    "            np.sum(pars_to_right - loc))\n",
    "\n",
    "def check_unique(mat, prev):\n",
    "    '''Removes the rows in mat that\n",
    "    are also in prev'''\n",
    "    for i, row in enumerate(mat):\n",
    "        if (prev==row).all(axis=1).any():\n",
    "            mat[i,:] = np.zeros(\n",
    "                mat.shape[1])\n",
    "    mat = mat[~np.all(\n",
    "        mat==0, axis=1)]\n",
    "    return mat\n",
    "\n",
    "def quick_best_order(arr, df, deps, count, prev):\n",
    "    '''Returns an approximation for the optimal\n",
    "    order.'''\n",
    "    count += 1\n",
    "    transposes = adj_transposes(arr)\n",
    "    transposes = check_unique(transposes, prev)\n",
    "    least_disarrayed = find_least_disarrayed(transposes, deps)\n",
    "    expected_times = get_expected_times(least_disarrayed, df)\n",
    "    best = least_disarrayed[np.argmin(expected_times)]\n",
    "    prev = np.append(prev, np.expand_dims(best, 0), 0)\n",
    "    if check_perm(best, deps)==True:\n",
    "        return best\n",
    "    elif count<50:\n",
    "        return quick_best_order(best, df, deps, count, prev)\n",
    "    else:\n",
    "        return (\"Error: Make sure dependencies don't produce\"\n",
    "                + \" a contradiction.\")\n",
    "\n",
    "def present_list(arr, df):\n",
    "    name_list = []\n",
    "    index_to_name = dict(zip( df.index + 1, df['Task']))\n",
    "    for index in arr:\n",
    "        name_list.append(index_to_name[index])\n",
    "    return pd.DataFrame(name_list, columns=[\"Task\"])\n",
    "\n",
    "    \n",
    "def plot_fig(sorted_sums, hist_vals):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    axs[0].plot(np.arange(len(sorted_sums)), sorted_sums, color='black', ls='--')\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_ylabel('Time')\n",
    "    axs[0].set_title('Expected time with respect to \\n order of tasks')\n",
    "    axs[1].hist(hist_vals, bins=30)\n",
    "    axs[1].set_title('Approximate distribution of occurence of failure')\n",
    "    axs[1].set_xlabel(\"Time\")\n",
    "    axs[1].set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "def plot_hist(trials):\n",
    "    fig, axs = plt.subplots()\n",
    "\n",
    "def simulate(df, index):\n",
    "    totals = []\n",
    "    prob = df['Probability'].values.astype(float)[index]\n",
    "    time = df['Time'].values.astype(float)[index]\n",
    "    lambdas = get_lambdas(time, prob)\n",
    "    success_count = 10000\n",
    "    prior_task_time = 0\n",
    "    for j, rate in enumerate(lambdas):\n",
    "        trials = RV(Exponential(rate=rate)).sim(success_count)\n",
    "        failed = trials.filter_leq(time[j])\n",
    "        totals.append(list(np.array(list(failed)) + prior_task_time))\n",
    "        success = trials.filter_geq(time[j])\n",
    "        success_count = len(success)\n",
    "        prior_task_time += time[j]\n",
    "    totals = flatten(totals)\n",
    "    return totals\n",
    "\n",
    "def get_lambdas(time, prob):\n",
    "    return 1/time*np.log(1/(1 - prob))    \n",
    "\n",
    "def flatten(int_list): \n",
    "    str_rep = str(list(int_list)).replace('[', '').replace(']', '')    \n",
    "    return [float(x) for x in str_rep.split(',') if x.strip]\n",
    "\n",
    "def main(df):\n",
    "    df = switch_from_names(df)\n",
    "    deps = df.filter(like='D').values\n",
    "    times = df['Time'].values.astype(float)\n",
    "    probs = df['Probability'].values.astype(float)\n",
    "    srtd_rates = sort_rates(times, probs)\n",
    "    prev = np.zeros([1,df.shape[0]])\n",
    "    rows = df.shape[0]\n",
    "    if rows < 10:\n",
    "        perms = gen_perms(df)\n",
    "        valid_perms = validate_perms(perms, deps)\n",
    "        sums = get_expected_times(valid_perms, df)\n",
    "        sorted_sums = np.sort(sums)\n",
    "        best_order = valid_perms[np.argmin(sums)]\n",
    "        best_df = present_list(best_order, df)\n",
    "        best_index = best_df.replace(\n",
    "            names_to_index_dict(df)).values.flatten() - 1\n",
    "        hist_vals = simulate(df, list(best_index))\n",
    "        printmd(\"## Report\")\n",
    "        printmd(\"##### The optimal order is \")\n",
    "        display(best_df)\n",
    "        printmd(\"##### the expected time you will work on this project is\")\n",
    "        printmd(\"##### \" + str(sorted_sums[0]))\n",
    "        printmd(\"##### and the probability you will succeed is\")\n",
    "        printmd(\"##### \" + str(np.prod(1 - probs)) + \".\")\n",
    "        plot_fig(sorted_sums, hist_vals)\n",
    "\n",
    "    else:\n",
    "        best = quick_best_order(srtd_rates, df, deps, 0, prev)\n",
    "        print('The optimal order is') \n",
    "        display(present_list(best, df))\n",
    "        print(\"and the expected time is\")\n",
    "        display(get_expected_times(\n",
    "                    np.expand_dims(best, 0), df)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it works:\n",
    "1. If you have previously used this app to build a sheet and you have the file saved, you can pick up where you left off by uploading it. Simply upload the file and click Build Sheet.\n",
    "2. To start with a blank sheet, just click Build Sheet.\n",
    "3. Fill the Tasks column with short names for each task, the Time column with your estimation for how long each task will take, and the Probability column with your estimation of the probability that each task will fail.\n",
    "4. Let's say you put task A in row 1. You would then put the name of any tasks that you must complete before you can attempt task A in the D columns on row 1.\n",
    "5. Click run to get a full report!\n",
    "6. Click the link at the bottom of the report to download your sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bbb7bdfed543498c0c94bf62244eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myupload = ipywidgets.FileUpload(accept='.csv', multiple=False)\n",
    "display(myupload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8904aeadc1a4a76816c61efa7d00dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Build Sheet', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sheet_butt = ipywidgets.Button(description='Build Sheet')\n",
    "out = ipywidgets.Output()\n",
    "sheet = build_sheet()\n",
    "\n",
    "def sheet_butt_func(_):\n",
    "    global sheet\n",
    "    with out:\n",
    "        try:\n",
    "            clear_output()\n",
    "            uploaded_filename = next(iter(myupload.value))\n",
    "            content = myupload.value[uploaded_filename]['content']\n",
    "            with open('task_sheet.csv', 'wb') as f: f.write(content)\n",
    "            sheet = build_sheet()\n",
    "            display(sheet)\n",
    "        except:\n",
    "            sheet = build_sheet()\n",
    "            display(sheet)\n",
    "sheet_butt.on_click(sheet_butt_func)\n",
    "display(ipywidgets.VBox([sheet_butt,out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c5e279126e48f2879265c413898ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Run', style=ButtonStyle()),)), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = ipywidgets.Button(description='Run')\n",
    "out = ipywidgets.Output()\n",
    "def run_program(_):\n",
    "    with out:\n",
    "        df = df_from_sheet(sheet)\n",
    "        df.to_csv('task_sheet.csv', index=False)\n",
    "        local_file = FileLink('./task_sheet.csv', result_html_prefix=\"Click here to download the sheet: \")\n",
    "        return main(df), display(local_file)\n",
    "        \n",
    "run.on_click(run_program)\n",
    "buttons = ipywidgets.HBox([run])\n",
    "ipywidgets.VBox([buttons,out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by letting $T$ be a random variable representing the time we spend on the project, from the begining to the point where we either succeed or fail. Our goal is to minimize $\\text{E}[T]$.\n",
    "\n",
    "Let $L$ be a random variable indicating the task we fail on. We define $L$ as\n",
    "\n",
    "$$\n",
    "L = \\begin{cases} \n",
    "     i & \\text{we fail on task }i \\\\\n",
    "     0 & \\text{we succeed}. \\\\ \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "We can now invoke the law of total expectation.\n",
    "\n",
    "$$\n",
    "\\text{E}[T] = \\text{E}[\\text{E}[T \\vert L]] = \\text{P}(L = 0)\\text{E}[T \\vert L = 0] + \\text{P}(L = 1)\\text{E}[T \\vert L = 1] + \\dots + \\text{P}(L = n)\\text{E}[T \\vert L = n].\n",
    "$$\n",
    "\n",
    "The user supplies us with their estimates of the probability of failure on each task, and their estimate for the time each task will take. We denote these as $p_1, \\dots p_n$ and $t_1, \\dots t_n$. For our purposes these are constant known values. It follows that\n",
    "\n",
    "$$\n",
    "\\text{P}(L = 0) = (1 - p_1) \\dots (1 - p_n) = \\prod_{i = 1}^n (1 - p_i) \\text{ and}\n",
    "$$\n",
    "$$\n",
    "\\text{E}[T \\vert L = 0] = t_1 + \\dots t_n = \\sum_{i = 1}^n t_i.\n",
    "$$\n",
    "Take $ i \\in [n]$. It's easy to see that\n",
    "$$\n",
    "\\text{P}(L = i) = (1 - p_1) \\dots (1 - p_{i - 1}) p_i = \\prod_{k = 0}^ {i - 1} (1 - p_k)p_i.\n",
    "$$\n",
    "\n",
    "It is not obvious what $E[T \\vert L_{i}]$ should be, and that is because we lack enough information about the distribution of $T$. We have to make an assumption, and the one we choose is that the probability of failing during the next minute of doing a task is independent of how long we have been doing the task (see <a href = https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html>Research as a Stocastic Decision Process</a>). This implies that $f_T$ is given by\n",
    "\n",
    "$$\n",
    "f_T(t) = \\begin{cases}\n",
    "            \\lambda_1 e^{-\\lambda_1 t} & 0 \\leq t < t_1 \\\\\n",
    "            (1 - p_1) \\lambda_2 e^{-\\lambda_2 (t - t_1)} & t_1 \\leq t < t_1 + t_2 \\\\\n",
    "            \\vdots & \\vdots \\\\\n",
    "           (1 - p_{i - 1}) \\dots (1 - p_1) \\lambda_i e^{-\\lambda_i( t -  t_1 - \\dots - t_{i - 1})} & t_1 + \\dots + t_{i - 1} \\leq t < t_1 + \\dots + t_{i} \\\\\n",
    "            \\vdots & \\vdots \\\\\n",
    "            (1 - p_{n}) \\dots (1 - p_{i - 1}) \\dots (1 - p_1) \\lambda_n e^{-\\lambda_n (t - t_1 - \\dots -t_{n - 1})} & t_1 + \\dots t_{n - 1} \\leq t < t_1  + \\dots t_{n},\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{i}$ is the failure rate of task $i$ given by\n",
    "$$\n",
    "\\lambda_{i} = \\frac{1}{t_i}\\ln \\Big{(}\\frac{1}{1 - p_{i}}\\Big{)}.\n",
    "$$\n",
    "\n",
    "From this we have\n",
    "$$\n",
    "f_{T \\vert \\{L = i\\}}(t) = \\frac{f_T(t)}{\\text{P}(L = i)} = \\frac{(1 - p_{i - 1}) \\dots (1 - p_1) \\lambda_i e^{-\\lambda_i t -  t_1 - \\dots - t_{i - 1}}}{(1 - p_{i - 1}) \\dots (1 - p_1)p_i} = \\frac{ \\lambda_i e^{-\\lambda_i t -  t_1 - \\dots - t_{i - 1}}}{p_i} \\text{ when } t_1 + \\dots + t_{i - 1} \\leq t < t_1 + \\dots + t_{i}.\n",
    "$$\n",
    "\n",
    "Let $t_0 = t_1 + \\dots + t_{i - 1}$. By performing a change of variables $ t \\rightarrow t - t_0$ we have\n",
    "$$\n",
    "E[T \\vert L = i] = \\frac{1}{p_i} \\int_0^{t_i} (t + t_0) \\lambda_i e^{-\\lambda_i t} dt,\n",
    "$$\n",
    "\n",
    "which after some simplification yields\n",
    "\n",
    "$$\n",
    "\\text{E}[T \\vert L = i] = \\frac{1 + \\lambda_i t_0 - e^{- \\lambda_i t_i } - \\lambda_i e^{ -\\lambda_i t_i } (t_0 + t_i)}{ \\lambda_i p_i}.\n",
    "$$\n",
    "\n",
    "Substituting in our value for $\\lambda_i$ and simplifying we have\n",
    "\n",
    "$$\n",
    "\\text{E}[T \\vert L = i] = t_0 - \\frac{t_i}{p_i} + \\frac{t_i}{\\ln \\big{(} \\frac{1}{1 - p_i} \\big{)} } + t_i.\n",
    "$$\n",
    "\n",
    "Combining this with what we found for $\\text{P}(L = i)$ and recalling that $t_0 = \\sum_{j = 1}^{i - 1} t_j$ we have\n",
    "\n",
    "$$\n",
    "\\text{P}(L = i)\\text{E}[T \\vert L = i] =  \\prod_{k = 1}^ {i - 1} (1 - p_k)p_i \\Bigg(\\sum_{j = 0}^{i - 1} t_j - \\frac{t_i}{p_i} + \\frac{t_i}{\\ln \\big{(} \\frac{1}{1 - p_i} \\big{)} } + t_i \\Bigg).\n",
    "$$\n",
    "\n",
    "Thus, our final expression for $ \\text{E}[T]$ is\n",
    "\n",
    "$$\n",
    "\\text{E}[T] = \\prod_{i = 1}^n (1 - p_i)\\sum_{i = 1}^n t_i + \\sum_{i = 1}^n \\Bigg( \\prod_{k = 1}^ {i - 1} (1 - p_k)p_i \\Bigg(\\sum_{j = 0}^{i - 1} t_j - \\frac{t_i}{p_i} + \\frac{t_i}{\\ln \\big{(} \\frac{1}{1 - p_i} \\big{)} } + t_i \\Bigg) \\Bigg).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
